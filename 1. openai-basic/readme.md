# Fundamentals of OpenAI and Basic Chat App

### 1. **OpenAI**

#### 1.1 Setup OpenAI Account

- follow this doc to create api key and run first prompt
- https://platform.openai.com/docs/quickstart?context=node

#### 1.2 What is a Context Window

- A context window refers to the amount of text/tokens that the LLM model can consider at a time while generating output. It refers to the input that model can see and understand the context while generating output. The size of context is usually measured in tokens. The larger the context window, the more probablity of contextually correct response from the LLM. If the input exceeds the context window, the LLM model would neglect the earlier part of the text which is the loss of the context, resuling in ineffecient outputs. GPT-4 turbo has 128,000 **tokens** context window.

#### 1.3 What is a token and tokenization

- A token is a piece of text, puncturation or other characters. It can be short as single character and long as one word. The tokenization is a process of breaking down a text into small units that model can understand and generate output with. For example, "unhappiness" might be tokenized into "un", "happi", and "ness" (3 tokens)
- https://platform.openai.com/tokenizer
  - #### 1.3.1 How to check tokens and cost prior to making API call?
    - tiktoken library is used to encode the text into tokens, we can estimate the no of tokens being used ulitized by the model as well as the cost of API calls before making the API call.
    - https://www.datacamp.com/tutorial/estimating-cost-of-gpt-using-tiktoken-library-python
    - OR we can just use OpenAI tokenizer to do all the same things:
    - https://platform.openai.com/tokenizer

#### 1.4 What are the roles in OpenAI APIs.

- Roles provide a structed way to communicate with the model. commonly used roles include:
  1. **system** - To setup context/behavor of model. Provide guidelines on how the conversation should proceed. You can use this to instruct the model to maintain the formal tone throughout the conversation, or to specify rules like, avoid centain topics.
  2. **user** - To represent human user, the prompts that user asks in a form of question/query or making a statement.
  3. **assistant** - To respresent the model itself, responding to the prompt based on the context set by the system.
  4. **tool** - to respresent the response from the function calling that the model requires to product it's next output. (check openai-function directory code for more understanding)

#### 1.5 What is a temperature

- It is used to control the randomness of the output. Its value can range between 0 to 2. When you set it towards 0, the output is more predictable and deterministic, and when you set it towards 2, the output is more random. Generally the random values are between 0.7 - 1 depands on the model.

#### 1.6 What is Top p

- The “top_p” parameter ranges from 0 to 1, is like a filter that decides how many possible words to consider. A high “top_p” value means the model looks at more possible words, even the less likely ones, which makes the generated text more diverse. Ex: “top p” value to 0.5, the language model will only consider the 50 most likely words or phrases that might come next. But if you set the “top p” value to 0.9, the language model will consider the 90 most likely words or phrases.

#### 1.7 What is Maximum Tokens

- An indicator to set the maxmium tokens to be used by the model.

#### 1.6 What is Frequency penalty

- The frequency penalty addresses a common problem in text generation: repetition. By applying penalties to frequently appearing words, the model is encouraged to diversify language use. The more the number, the higher the penalty

### No of choices.

- the number of output choices generated by the model.

### 2. **Chat Application using OpenAI**

We will be using terminal as our chatting platform. The source code is inside src directory well commented explaining very bit.

#### 2.1 Chat app overview

- We used node readable stream to `process.stdin.addListener` method to listen to the messages that we type inside the terminal.
- To make out chat relavent, we have setup a context array that keep all the messages of the coversation.
- To limit our context, we have setup a MAX_TOKEN limit that helps us in keeping the context within the boundaries of the model's context window.

### Imporatnt resources:

- https://www.prompthub.us/blog/understanding-openai-parameters-how-to-optimize-your-prompts-for-better-outputs#:~:text=The%20higher%20the%20frequency%20penalty,Example
-
